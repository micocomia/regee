{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened PDF file\n",
      "Successfully opened PPTX file with 23 slides\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import streamlit as st\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Processes documents (PDF, PPTX) for the review chatbot.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\", \n",
    "                 chunk_size: int = 500, chunk_overlap: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the document processor.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Name of the sentence-transformers model to use\n",
    "            chunk_size: Size of document chunks in characters\n",
    "            chunk_overlap: Overlap between chunks in characters\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a document file and return chunks with embeddings.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            List of document chunks with content, embeddings, and metadata\n",
    "        \"\"\"\n",
    "        # Extract text based on file type\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            page_texts = self._extract_pdf_text(file_path)\n",
    "            # Extract topics from the combined text of all pages\n",
    "            full_text = ' '.join([page_info['text'] for page_info in page_texts])\n",
    "            topics = self._extract_topics(full_text)\n",
    "            \n",
    "            # Process each page and track page numbers\n",
    "            processed_chunks = []\n",
    "            chunk_id = 0\n",
    "            \n",
    "            for page_info in page_texts:\n",
    "                page_text = page_info['text']\n",
    "                page_num = page_info['page_number']\n",
    "                \n",
    "                # Chunk the page text\n",
    "                chunks = self._chunk_text(page_text)\n",
    "                \n",
    "                # Create embeddings for each chunk from this page\n",
    "                for chunk in chunks:\n",
    "                    embedding = self.embedding_model.encode(chunk, show_progress_bar=False)\n",
    "                    \n",
    "                    processed_chunks.append({\n",
    "                        'content': chunk,\n",
    "                        'embedding': embedding,\n",
    "                        'metadata': {\n",
    "                            'source': os.path.basename(file_path),\n",
    "                            'chunk_id': chunk_id,\n",
    "                            'topics': topics,\n",
    "                            'page_number': page_num\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            return processed_chunks\n",
    "            \n",
    "        elif file_extension in ['.pptx', '.ppt']:\n",
    "            slide_texts = self._extract_pptx_text(file_path)\n",
    "            # Extract topics from the combined text of all slides\n",
    "            full_text = ' '.join([slide_info['text'] for slide_info in slide_texts])\n",
    "            topics = self._extract_topics(full_text)\n",
    "            \n",
    "            # Process each slide and track slide numbers\n",
    "            processed_chunks = []\n",
    "            chunk_id = 0\n",
    "            \n",
    "            for slide_info in slide_texts:\n",
    "                slide_text = slide_info['text']\n",
    "                slide_num = slide_info['slide_number']\n",
    "                \n",
    "                # Chunk the slide text\n",
    "                chunks = self._chunk_text(slide_text)\n",
    "                \n",
    "                # Create embeddings for each chunk from this slide\n",
    "                for chunk in chunks:\n",
    "                    embedding = self.embedding_model.encode(chunk, show_progress_bar=False)\n",
    "                    \n",
    "                    processed_chunks.append({\n",
    "                        'content': chunk,\n",
    "                        'embedding': embedding,\n",
    "                        'metadata': {\n",
    "                            'source': os.path.basename(file_path),\n",
    "                            'chunk_id': chunk_id,\n",
    "                            'topics': topics,\n",
    "                            'page_number': slide_num  # For slides, use slide number as page number\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            return processed_chunks\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "    def _extract_pdf_text(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text from PDF file with page tracking.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with text content and page number\n",
    "        \"\"\"\n",
    "        page_texts = []\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                text = page.extract_text()\n",
    "                if text.strip():  # Only add non-empty pages\n",
    "                    page_texts.append({\n",
    "                        'text': text,\n",
    "                        'page_number': page_num + 1  \n",
    "                    })\n",
    "        return page_texts\n",
    "    \n",
    "    def _extract_pptx_text(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text from PowerPoint file with slide tracking.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with text content and slide number\n",
    "        \"\"\"\n",
    "        slide_texts = []\n",
    "        prs = Presentation(file_path)\n",
    "        for slide_num, slide in enumerate(prs.slides):\n",
    "            text = \"\"\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "            \n",
    "            if text.strip():  # Only add non-empty slides\n",
    "                slide_texts.append({\n",
    "                    'text': text,\n",
    "                    'slide_number': slide_num + 1  \n",
    "                })\n",
    "        return slide_texts\n",
    "    \n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            \n",
    "            # Adjust end to avoid splitting words\n",
    "            if end < len(text):\n",
    "                # Look for the last space within the chunk\n",
    "                last_space = text.rfind(' ', start, end)\n",
    "                if last_space != -1 and last_space > start:\n",
    "                   end = last_space + 1  # Include the space\n",
    "\n",
    "            # Add the chunk\n",
    "            chunks.append(text[start:end])\n",
    "            \n",
    "            # Move the start position for the next chunk, considering overlap\n",
    "            start = max(end - self.chunk_overlap, start + 1)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_topics(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key topics from text (simple version).\n",
    "        A more sophisticated topic extraction would be implemented here.\n",
    "        \"\"\"\n",
    "        # Simple keyword extraction - in a real system, use TF-IDF or LDA\n",
    "        common_words = ['the', 'and', 'or', 'to', 'a', 'in', 'that', 'it', 'with']\n",
    "        words = [word.lower() for word in text.split() if len(word) > 4]\n",
    "        word_counts = {}\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in common_words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Get the top 5 words as \"topics\"\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        topics = [word for word, count in sorted_words[:5]]\n",
    "        \n",
    "        return topics\n",
    "\n",
    "# Integration code for Streamlit app\n",
    "def process_uploaded_file(uploaded_file):\n",
    "    \"\"\"Process an uploaded document.\"\"\"\n",
    "    # Save the file temporarily\n",
    "    file_path = os.path.join(\"./uploads\", uploaded_file.name)\n",
    "    os.makedirs(\"./uploads\", exist_ok=True)\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.getbuffer())\n",
    "    \n",
    "    # Process the document\n",
    "    document_chunks = document_processor.process_document(file_path)\n",
    "    \n",
    "    # Add to vector store\n",
    "    vector_store.add_documents(document_chunks)\n",
    "    \n",
    "    # Extract topics for the UI\n",
    "    all_topics = set()\n",
    "    for chunk in document_chunks:\n",
    "        all_topics.update(chunk['metadata']['topics'])\n",
    "    \n",
    "    # Update session state\n",
    "    st.session_state.documents.append(file_path)\n",
    "    st.session_state.document_names.append(uploaded_file.name)\n",
    "    \n",
    "    for topic in all_topics:\n",
    "        if topic not in st.session_state.topics:\n",
    "            st.session_state.topics.append(topic)\n",
    "            \n",
    "    return True\n",
    "\n",
    "# Test the processor\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "# Test with a PDF file\n",
    "pdf_path = r\"C:\\Users\\karel\\Downloads\\answer-generation-for-retrieval-based-question-answering-systems.pdf\" \n",
    "# Test if files can be opened\n",
    "try:\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        print(\"Successfully opened PDF file\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to open PDF: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation\n",
    "    prs = Presentation(pptx_path)\n",
    "    print(f\"Successfully opened PPTX file with {len(prs.slides)} slides\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to open PPTX: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DocumentProcessor...\n",
      "Testing PDF processing: C:\\Users\\karel\\Downloads\\answer-generation-for-retrieval-based-question-answering-systems.pdf\n",
      "Successfully processed PDF into 775 chunks\n",
      "First chunk content: Answer Generation for Retrieval-based Question Answering Systems\n",
      "Chao-Chun Hsu1\u0003, Eric Lind2, Luca S...\n",
      "Topics identified: ['answer', 'model', 'genqa', 'association', 'pages']\n",
      "Page number: 1\n",
      "\n",
      "Testing PowerPoint processing: C:\\Users\\karel\\Downloads\\DTI 5125 Question Answering Group 1.pptx\n",
      "Successfully processed PowerPoint into 1835 chunks\n",
      "First chunk content: Article presentation:\n",
      "Question answering SYSTEMS\n",
      "Akshat Khare - 300342170\n",
      "Laurie Kanga-Eba - 3004331...\n",
      "Topics identified: ['answer', 'model', 'question', 'article', 'answering']\n",
      "Slide number: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the processor\n",
    "print(\"Initializing DocumentProcessor...\")\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "# Test with PDF file\n",
    "pdf_path = r\"C:\\Users\\karel\\Downloads\\answer-generation-for-retrieval-based-question-answering-systems.pdf\"\n",
    "print(f\"Testing PDF processing: {pdf_path}\")\n",
    "try:\n",
    "    pdf_chunks = processor.process_document(pdf_path)\n",
    "    print(f\"Successfully processed PDF into {len(pdf_chunks)} chunks\")\n",
    "    if pdf_chunks:\n",
    "        print(f\"First chunk content: {pdf_chunks[0]['content'][:100]}...\")\n",
    "        print(f\"Topics identified: {pdf_chunks[0]['metadata']['topics']}\")\n",
    "        print(f\"Page number: {pdf_chunks[0]['metadata']['page_number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing PDF: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test with PowerPoint file\n",
    "pptx_path = r\"C:\\Users\\karel\\Downloads\\DTI 5125 Question Answering Group 1.pptx\"\n",
    "print(f\"\\nTesting PowerPoint processing: {pptx_path}\")\n",
    "try:\n",
    "    pptx_chunks = processor.process_document(pptx_path)\n",
    "    print(f\"Successfully processed PowerPoint into {len(pptx_chunks)} chunks\")\n",
    "    if pptx_chunks:\n",
    "        print(f\"First chunk content: {pptx_chunks[0]['content'][:100]}...\")\n",
    "        print(f\"Topics identified: {pptx_chunks[0]['metadata']['topics']}\")\n",
    "        print(f\"Slide number: {pptx_chunks[0]['metadata']['page_number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing PPTX: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
