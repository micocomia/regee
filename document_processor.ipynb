{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-pptx\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-pptx) (10.3.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
      "  Using cached XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting lxml>=3.1.0 (from python-pptx)\n",
      "  Using cached lxml-5.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-pptx) (4.11.0)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 28.4 MB/s eta 0:00:00\n",
      "Using cached XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
      "Installing collected packages: XlsxWriter, lxml, python-pptx\n",
      "Successfully installed XlsxWriter-3.2.2 lxml-5.3.1 python-pptx-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.48.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\karel\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\karel\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n",
      "Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.43.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\karel\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (24.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (3.20.2)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\karel\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\karel\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.43.2-py2.py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 28.9 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 731.2/731.2 kB 15.1 MB/s eta 0:00:00\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.9/6.9 MB 35.5 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading narwhals-1.31.0-py3-none-any.whl (313 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, pyarrow, narwhals, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 narwhals-1.31.0 pyarrow-19.0.1 pydeck-0.9.1 smmap-5.0.2 streamlit-1.43.2 tenacity-9.0.0 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import streamlit as st\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Processes documents (PDF, PPTX) for the review chatbot.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\", \n",
    "                 chunk_size: int = 500, chunk_overlap: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize the document processor.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Name of the sentence-transformers model to use\n",
    "            chunk_size: Size of document chunks in characters\n",
    "            chunk_overlap: Overlap between chunks in characters\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def process_document(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a document file and return chunks with embeddings.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            List of document chunks with content, embeddings, and metadata\n",
    "        \"\"\"\n",
    "        # Extract text based on file type\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            page_texts = self._extract_pdf_text(file_path)\n",
    "            # Extract topics from the combined text of all pages\n",
    "            full_text = ' '.join([page_info['text'] for page_info in page_texts])\n",
    "            topics = self._extract_topics(full_text)\n",
    "            \n",
    "            # Process each page and track page numbers\n",
    "            processed_chunks = []\n",
    "            chunk_id = 0\n",
    "            \n",
    "            for page_info in page_texts:\n",
    "                page_text = page_info['text']\n",
    "                page_num = page_info['page_number']\n",
    "                \n",
    "                # Chunk the page text\n",
    "                chunks = self._chunk_text(page_text)\n",
    "                \n",
    "                # Create embeddings for each chunk from this page\n",
    "                for chunk in chunks:\n",
    "                    embedding = self.embedding_model.encode(chunk, show_progress_bar=False)\n",
    "                    \n",
    "                    processed_chunks.append({\n",
    "                        'content': chunk,\n",
    "                        'embedding': embedding,\n",
    "                        'metadata': {\n",
    "                            'source': os.path.basename(file_path),\n",
    "                            'chunk_id': chunk_id,\n",
    "                            'topics': topics,\n",
    "                            'page_number': page_num\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            return processed_chunks\n",
    "            \n",
    "        elif file_extension in ['.pptx', '.ppt']:\n",
    "            slide_texts = self._extract_pptx_text(file_path)\n",
    "            # Extract topics from the combined text of all slides\n",
    "            full_text = ' '.join([slide_info['text'] for slide_info in slide_texts])\n",
    "            topics = self._extract_topics(full_text)\n",
    "            \n",
    "            # Process each slide and track slide numbers\n",
    "            processed_chunks = []\n",
    "            chunk_id = 0\n",
    "            \n",
    "            for slide_info in slide_texts:\n",
    "                slide_text = slide_info['text']\n",
    "                slide_num = slide_info['slide_number']\n",
    "                \n",
    "                # Chunk the slide text\n",
    "                chunks = self._chunk_text(slide_text)\n",
    "                \n",
    "                # Create embeddings for each chunk from this slide\n",
    "                for chunk in chunks:\n",
    "                    embedding = self.embedding_model.encode(chunk, show_progress_bar=False)\n",
    "                    \n",
    "                    processed_chunks.append({\n",
    "                        'content': chunk,\n",
    "                        'embedding': embedding,\n",
    "                        'metadata': {\n",
    "                            'source': os.path.basename(file_path),\n",
    "                            'chunk_id': chunk_id,\n",
    "                            'topics': topics,\n",
    "                            'page_number': slide_num  # For slides, use slide number as page number\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            return processed_chunks\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "    def _extract_pdf_text(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text from PDF file with page tracking.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with text content and page number\n",
    "        \"\"\"\n",
    "        page_texts = []\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                text = page.extract_text()\n",
    "                if text.strip():  # Only add non-empty pages\n",
    "                    page_texts.append({\n",
    "                        'text': text,\n",
    "                        'page_number': page_num + 1  \n",
    "                    })\n",
    "        return page_texts\n",
    "    \n",
    "    def _extract_pptx_text(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text from PowerPoint file with slide tracking.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with text content and slide number\n",
    "        \"\"\"\n",
    "        slide_texts = []\n",
    "        prs = Presentation(file_path)\n",
    "        for slide_num, slide in enumerate(prs.slides):\n",
    "            text = \"\"\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "            \n",
    "            if text.strip():  # Only add non-empty slides\n",
    "                slide_texts.append({\n",
    "                    'text': text,\n",
    "                    'slide_number': slide_num + 1  \n",
    "                })\n",
    "        return slide_texts\n",
    "    \n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks with overlap.\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            \n",
    "            # Adjust end to avoid splitting words\n",
    "            if end < len(text):\n",
    "                # Look for the last space within the chunk\n",
    "                last_space = text.rfind(' ', start, end)\n",
    "                if last_space != -1 and last_space > start:\n",
    "                   end = last_space + 1  # Include the space\n",
    "\n",
    "            # Add the chunk\n",
    "            chunks.append(text[start:end])\n",
    "            \n",
    "            # Move the start position for the next chunk, considering overlap\n",
    "            start = max(end - self.chunk_overlap, start + 1)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_topics(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract key topics from text (simple version).\n",
    "        A more sophisticated topic extraction would be implemented here.\n",
    "        \"\"\"\n",
    "        # Simple keyword extraction - in a real system, use TF-IDF or LDA\n",
    "        common_words = ['the', 'and', 'or', 'to', 'a', 'in', 'that', 'it', 'with']\n",
    "        words = [word.lower() for word in text.split() if len(word) > 4]\n",
    "        word_counts = {}\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in common_words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Get the top 5 words as \"topics\"\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        topics = [word for word, count in sorted_words[:5]]\n",
    "        \n",
    "        return topics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DocumentProcessor...\n",
      "Testing PDF processing: C:\\Users\\karel\\Downloads\\answer-generation-for-retrieval-based-question-answering-systems.pdf\n",
      "Successfully processed PDF into 775 chunks\n",
      "First chunk content: Answer Generation for Retrieval-based Question Answering Systems\n",
      "Chao-Chun Hsu1\u0003, Eric Lind2, Luca S...\n",
      "Topics identified: ['answer', 'model', 'genqa', 'association', 'pages']\n",
      "Page number: 1\n",
      "\n",
      "Testing PowerPoint processing: C:\\Users\\karel\\Downloads\\DTI 5125 Question Answering Group 1.pptx\n",
      "Successfully processed PowerPoint into 1835 chunks\n",
      "First chunk content: Article presentation:\n",
      "Question answering SYSTEMS\n",
      "Akshat Khare - 300342170\n",
      "Laurie Kanga-Eba - 3004331...\n",
      "Topics identified: ['answer', 'model', 'question', 'article', 'answering']\n",
      "Slide number: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the processor\n",
    "print(\"Initializing DocumentProcessor...\")\n",
    "processor = DocumentProcessor()\n",
    "\n",
    "# Test with PDF file\n",
    "pdf_path = r\"C:\\Users\\karel\\Downloads\\answer-generation-for-retrieval-based-question-answering-systems.pdf\"\n",
    "print(f\"Testing PDF processing: {pdf_path}\")\n",
    "try:\n",
    "    pdf_chunks = processor.process_document(pdf_path)\n",
    "    print(f\"Successfully processed PDF into {len(pdf_chunks)} chunks\")\n",
    "    if pdf_chunks:\n",
    "        print(f\"First chunk content: {pdf_chunks[0]['content'][:100]}...\")\n",
    "        print(f\"Topics identified: {pdf_chunks[0]['metadata']['topics']}\")\n",
    "        print(f\"Page number: {pdf_chunks[0]['metadata']['page_number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing PDF: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test with PowerPoint file\n",
    "pptx_path = r\"C:\\Users\\karel\\Downloads\\DTI 5125 Question Answering Group 1.pptx\"\n",
    "print(f\"\\nTesting PowerPoint processing: {pptx_path}\")\n",
    "try:\n",
    "    pptx_chunks = processor.process_document(pptx_path)\n",
    "    print(f\"Successfully processed PowerPoint into {len(pptx_chunks)} chunks\")\n",
    "    if pptx_chunks:\n",
    "        print(f\"First chunk content: {pptx_chunks[0]['content'][:100]}...\")\n",
    "        print(f\"Topics identified: {pptx_chunks[0]['metadata']['topics']}\")\n",
    "        print(f\"Slide number: {pptx_chunks[0]['metadata']['page_number']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing PPTX: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
